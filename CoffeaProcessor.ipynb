{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coffea processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea import hist, util\n",
    "\n",
    "import coffea.processor as processor\n",
    "from coffea.nanoevents import NanoEventsFactory, NanoAODSchema\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import uproot\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below introduces some basic concepts for writing code using Coffea.\n",
    "\n",
    "There are three primary pieces to the Coffea code:\n",
    "\n",
    "The processor, which contains all of the analysis cuts and fills the histogram in the process function.\n",
    "The second cell defines the files we want to run over and then runs the code using run_uproot_job.\n",
    "After we run the processor, we can then plot any of the histograms we have generated.\n",
    "To test any changes you make to the histograms, you will have to rerun each of the three cells below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MuonSelector(processor.ProcessorABC):\n",
    "    def __init__(self):\n",
    "        # In the initializer, any of the outputs you would like to produce are defined (ex. histograms)\n",
    "\n",
    "        # Coffea histograms are defined in the same way as in the previous exercise\n",
    "        # define a list of axes first\n",
    "\n",
    "        #Declare an axis for the dataset\n",
    "        dataset_axis = hist.Cat(\"dataset\",\"Dataset\")\n",
    "        \n",
    "        #Declare an axis for the muon pt\n",
    "        pt_axis = hist.Bin(\"pt\",\"$p_{T}$ [GeV]\", 40, 0, 200)\n",
    "        eta_axis = hist.Bin(\"eta\",\"$\\eta$\", 50, -2.5, 2.5)\n",
    "        phi_axis = hist.Bin(\"phi\",\"$\\phi$\", 64, -3.2, 3.2)\n",
    "        \n",
    "        #Define the accumulator object, a dictionary storing all of the histograms and counters \n",
    "        #that we will fill later in the process function\n",
    "        self._accumulator = processor.dict_accumulator({\n",
    "            'muon_pt': hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            'muon_eta': hist.Hist(\"Counts\", dataset_axis, eta_axis),\n",
    "            'muon_phi': hist.Hist(\"Counts\", dataset_axis, phi_axis),\n",
    "\n",
    "            'jet0_pt': hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            'jet0_eta': hist.Hist(\"Counts\", dataset_axis, eta_axis),\n",
    "            'jet0_phi': hist.Hist(\"Counts\", dataset_axis, phi_axis),\n",
    "\n",
    "            'jet1_pt': hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            'jet1_eta': hist.Hist(\"Counts\", dataset_axis, eta_axis),\n",
    "            'jet1_phi': hist.Hist(\"Counts\", dataset_axis, phi_axis),\n",
    "        }\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "\n",
    "    \n",
    "\n",
    "    # The process method is where the heart of the analysis is.  \n",
    "    # This is where all of the selections are done and the histograms get filled \n",
    "    #  (things you did in notebook cells before will be done here instead)\n",
    "    def process(self, events):\n",
    "        ### The process function is where most of the work happens. As we'll see below, this is\n",
    "        ### where the main analysis work happens (object cuts, event selections, filling histograms). \n",
    "        \n",
    "        ## This gets us the accumulator dictionary we defined in init\n",
    "        output = self.accumulator.identity()\n",
    "\n",
    "        ## To access variables from the ntuples, use the \"events\" object\n",
    "        ## The dataset name is part of events.metadata\n",
    "        dataset = events.metadata['dataset']\n",
    "\n",
    "        ## The coffea NanoEventSchema packages all muon variables (columns) into the events.Muon object\n",
    "        ## Each variable can be accessed using muons.key_name\n",
    "        muons = events.Muon        \n",
    "        \n",
    "        ######\n",
    "        # Select muons with pt >30, eta < 2.4, tight ID, and relIso < 0.15\n",
    "        muonSelectTight = ((muons.pt>30) &\n",
    "                           (abs(muons.eta)<2.4) &\n",
    "                           (muons.tightId) &\n",
    "                           (muons.pfRelIso04_all < 0.15)\n",
    "                          )\n",
    "\n",
    "        # Apply the selection to muons using the array[mask] syntax. \n",
    "        # tightMuons only includes the muons that pass the tight selection we defined\n",
    "        tightMuons = muons[muonSelectTight]\n",
    "        \n",
    "        jets = events.Jet\n",
    "        \n",
    "        ######\n",
    "        # Select jets with pt >30, eta < 2.4, and tight ID\n",
    "        jetSelectTight = ((jets.pt>30) &\n",
    "                          (abs(jets.eta)<2.4) &\n",
    "                          (jets.isTight)\n",
    "                         )\n",
    "        tightJets = jets[jetSelectTight]\n",
    "\n",
    "        ######\n",
    "        # Select b-tagged jets\n",
    "        bjetSelectTight = (jetSelectTight &\n",
    "                           (jets.btagDeepB>0.6321)\n",
    "                          )\n",
    "        tightBJets = jets[bjetSelectTight]\n",
    "        \n",
    "        \n",
    "        #Apply a High Level Trigger (HLT) requirement\n",
    "        #  look for events which passed a single muon trigger, either IsoMu24 or IsoTkMu24\n",
    "        #  This is a separate branch in the tree, and is stored in NanoEvents under an HLT object\n",
    "        \n",
    "        trigger = events.HLT.IsoMu24 | events.HLT.IsoTkMu24\n",
    "        \n",
    "        # Select events passing the trigger, with exactly on tight muon, ≥4 jets, and ≥ 1 b-tagged jets. \n",
    "        eventSelection = (trigger &\n",
    "                          (ak.num(tightMuons)==1) &\n",
    "                          (ak.num(tightJets)>=4) & \n",
    "                          (ak.num(tightBJets)>=1))\n",
    "\n",
    "        # Fill the muon_pt histogram using the tightMuons in events that pass our selection \n",
    "        # Note that ak.flatten() is required when filling a histogram to remove the jaggedness\n",
    "        output['muon_pt'].fill(dataset=dataset,\n",
    "                              pt=ak.flatten(tightMuons[eventSelection].pt))\n",
    "        output['muon_eta'].fill(dataset=dataset,\n",
    "                              eta=ak.flatten(tightMuons[eventSelection].eta))\n",
    "        output['muon_phi'].fill(dataset=dataset,\n",
    "                              phi=ak.flatten(tightMuons[eventSelection].phi))\n",
    "\n",
    "        output['jet0_pt'].fill(dataset=dataset,\n",
    "                              pt=ak.flatten(tightJets[eventSelection,0:1].pt))\n",
    "        output['jet0_eta'].fill(dataset=dataset,\n",
    "                              eta=ak.flatten(tightJets[eventSelection,0:1].eta))\n",
    "        output['jet0_phi'].fill(dataset=dataset,\n",
    "                              phi=ak.flatten(tightJets[eventSelection,0:1].phi))\n",
    "\n",
    "        output['jet1_pt'].fill(dataset=dataset,\n",
    "                              pt=ak.flatten(tightJets[eventSelection,1:2].pt))\n",
    "        output['jet1_eta'].fill(dataset=dataset,\n",
    "                              eta=ak.flatten(tightJets[eventSelection,1:2].eta))\n",
    "        output['jet1_phi'].fill(dataset=dataset,\n",
    "                              phi=ak.flatten(tightJets[eventSelection,1:2].phi))\n",
    "\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The processor takes a dictionary of files to run on, with a key of the dataset name and a list of files as the items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define files to run over\n",
    "skimDir=\"/udrive/staff/dnoonan/Skims\"\n",
    "skimDir=\"root://cmseos.fnal.gov//store/user/lpctop/TTGamma_FullRun2/Skims_v6-2/2016/\"\n",
    "fileset = {\"TTGamma\":[f\"{skimDir}/TTGamma_SingleLept_2016_skim.root\"],\n",
    "           \"TTbar\":[f\"{skimDir}/TTbarPowheg_Semilept_2016_skim_1of10.root\",\n",
    "                    f\"{skimDir}/TTbarPowheg_Semilept_2016_skim_2of10.root\"],\n",
    "           \"WGamma\":[f\"{skimDir}/WGamma_2016_skim.root\"],\n",
    "           \"Z+jets\":[f'{skimDir}/DYjetsM50_ext2_2016_skim_1of10.root'],\n",
    "           \"W+3jets\":[f\"{skimDir}/W3jets_2016_skim.root\"],\n",
    "           \"W+4jets\":[f\"{skimDir}/W4jets_2016_skim.root\"],\n",
    "          }\n",
    "\n",
    "filesetData = {\"DataMu\":[f\"{skimDir}/Data_SingleMu_b_2016_skim_1of10.root\"],\n",
    "              }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell will run over all of the datasets you have selected.  It may take a few minutes to run, but this is because you are running over close to 20 million events of MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.warnings.filterwarnings('ignore')\n",
    "\n",
    "#the NanoAODSchema needs to be adjusted, to remove cross references to FSRPhotons\n",
    "class SkimmedSchema(NanoAODSchema):\n",
    "    def __init__(self, base_form):\n",
    "        base_form[\"contents\"].pop(\"Muon_fsrPhotonIdx\", None)\n",
    "        super().__init__(base_form)\n",
    "\n",
    "#Run Coffea code using uproot\n",
    "outputMC = processor.run_uproot_job(\n",
    "    fileset,  #dictionary of datasets to run on, defined earlier in this cell\n",
    "    \"Events\", #Name of the TTree you will be opening\n",
    "    MuonSelector(),  #Coffea processor you defined\n",
    "    processor.futures_executor,\n",
    "    executor_args={\"schema\": SkimmedSchema,'workers': 4},  ## workers = 2, parallelize jobs, running 2 at once\n",
    "    chunksize=1000000, #in each chunk, use 1 million events\n",
    "#     maxchunks=3, #limit to using only 3 chunks for each dataset (useful for testing purposes)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coffea histograms that are defined in the accumulator in the initializer\n",
    "\n",
    "ex:\n",
    "```\n",
    "self._accumulator = processor.dict_accumulator({\n",
    "      'muon_pt': hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "...\n",
    "```\n",
    "\n",
    "are what get returned when you run the processor.  In this case, they are stored as `outputMC`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputMC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.plot1d(outputMC['muon_pt'],overlay='dataset',stack=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll run over the actualy data to et something to compare against, and store the histogram outputs as `outputData`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run coffea processor again, this time using the filesetData list\n",
    "outputData = processor.run_uproot_job(\n",
    "    filesetData,\n",
    "    \"Events\",\n",
    "    MuonSelector(),\n",
    "    processor.futures_executor,\n",
    "    executor_args={\"schema\": SkimmedSchema,'workers': 4}, \n",
    "    chunksize=1000000,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can draw both the histograms from Monte-Carlo (saved in the outputMC) and the histogram of data (outputData) together on the same plot.\n",
    "\n",
    "This can be done by calling hist.plot1d twice within the same cell, which will make it draw on the same axes.\n",
    "\n",
    "We also add some different drawing options for the data histogram.  This makes it draw the data as black points, rather than filled histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.plot1d(outputMC['muon_pt'],overlay='dataset',stack=True)\n",
    "\n",
    "data_err_opts = {\n",
    "    'linestyle':'none',\n",
    "    'marker': '.',\n",
    "    'markersize': 10.,\n",
    "    'color':'k',\n",
    "    'elinewidth': 1,\n",
    "}\n",
    "\n",
    "hist.plot1d(outputData['muon_pt'],overlay='dataset',error_opts=data_err_opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that they do not agree at all, this is because we have not yet scaled the Monte-Carlo histograms to match the luminosity of the data\n",
    "\n",
    "### Histogram Scaling\n",
    "\n",
    "When comparing a Monte-Carlo to Data, we need to scale the MC to the number of events we expect to see in a given amount of data.\n",
    "\n",
    "$\\text{N}_\\text{expected} = \\sigma \\cdot L$\n",
    "\n",
    " - $\\text{N}_\\text{expected}$ = Number of events expected\n",
    " - $\\sigma$ = cross section of a specific process\n",
    " - $L$ = integrated luminosity of data\n",
    " \n",
    "In MC, we often generate far more events than we expect (for better statistical uncertainties), so we need rescale the MC distributions.  This is done by reweighting each MC dataset, where the weight applied is the ratio of the number of events expected to the number of events produced in the MC ($\\text{N}_{MC}$)\n",
    "\n",
    "$w = \\frac{\\text{N}_\\text{expected}}{\\text{N}_\\text{MC}} = \\frac{\\sigma \\cdot L}{\\text{N}_\\text{MC}}$\n",
    "\n",
    "The number of events in MC and the cross section will change for each dataset\n",
    "\n",
    "#### Cross sections\n",
    "| Process | Cross Section (pb) |\n",
    "| :--- | :---: |\n",
    "| TTGamma (single lepton) | 7.509 |\n",
    "| TTbar (single lepton) | 380.095 |\n",
    "| WGamma | 489 |\n",
    "| Z+jets | 6077.22 |\n",
    "| W+3jets | 1165.8108 |\n",
    "| W+4jets | 592.9176 |\n",
    "\n",
    "#### Number of events\n",
    "\n",
    "The $\\text{N}_\\text{MC}$ value should be the total number of \n",
    "Normally, in NanoAOD, you could keep track of the number of events are in the files that you process (tallying the total number of events in each sample, across all chunks processed).\n",
    "\n",
    "However, since we are running on skims of the full MC sets, some of the events have already been removed.  However, in this case, we get the \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the code below uses uproot to open a histogram in the root file, used to track the total\n",
    "#  number of events processed while producing the skim\n",
    "\n",
    "nEvents = {}\n",
    "for d in fileset:\n",
    "    if not d in nEvents:\n",
    "        nEvents[d] = 0\n",
    "    for fName in fileset[d]:\n",
    "        with uproot.open(fName)['hEvents'] as hEvents:\n",
    "            nEvents[d] += hEvents.values()[0] + hEvents.values()[2]\n",
    "pprint(nEvents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get something like:\n",
    "```\n",
    "{'TTGamma': 11005200.0,\n",
    " 'TTbar': 17673700.0,\n",
    " 'W+3jets': 19798117.0,\n",
    " 'W+4jets': 9116657.0,\n",
    " 'WGamma': 6103817.0,\n",
    " 'Z+jets': 8920292.0}\n",
    "\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate weights\n",
    "\n",
    "Make a dictionary, containing the weights to apply for each dataset in fileset\n",
    "The dictionary should have the same key names as are in fileset, since these are what get used as the 'dataset' in the histogram axis.\n",
    "\n",
    "The actual CMS data you are using corresponds to an integrated luminosity $L = 578 \\text{pb}^{-1}$ (10% of data collected in 2016 Run B, or a little less than 2% of the whole 2016 dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "## To Do\n",
    "## Make new dictionary named weights, containing the luminosity and cross section based weights for each sample\n",
    "###############\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cx = {'TTGamma':7.509,\n",
    "     'TTbar': 380.095,\n",
    "     'WGamma':489,\n",
    "     'Z+jets':6077.22,\n",
    "     'W+3jets':1165.8108,\n",
    "     'W+4jets':592.9176}\n",
    "lumi_weight = {}\n",
    "for keyName in fileset:\n",
    "    lumi_weight[keyName] = (cx[keyName]*578.)/nEvents[keyName]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(lumi_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get something like:\n",
    "```\n",
    "{'TTGamma': 0.00039437738523607027,\n",
    " 'TTbar': 0.012430612152520412,\n",
    " 'W+3jets': 0.034035491476285346,\n",
    " 'W+4jets': 0.037591232487961326,\n",
    " 'WGamma': 0.04630577882659326,\n",
    " 'Z+jets': 0.39378006459878223}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop over objects in the output, and scale them to the by the weight dictionary you just created\n",
    "for key, obj in outputMC.items():\n",
    "    if isinstance(obj, hist.Hist):\n",
    "        obj.scale(lumi_weight, axis=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.plot1d(outputData['muon_pt'],overlay='dataset',error_opts=data_err_opts,overflow='over')\n",
    "hist.plot1d(outputMC['muon_pt'],overlay='dataset',stack=True,overflow='over')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should look something like:\n",
    "    \n",
    "<img src=\"plots/processor_example_muonPt.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do\n",
    "\n",
    "Add histograms into the coffea processor for the following variables:\n",
    " - muon eta\n",
    " - muon phi\n",
    " - leading jet pt\n",
    " - leading jet eta\n",
    " - leading jet phi\n",
    " - second leading jet pt\n",
    " - second leading jet eta\n",
    " - second leading jet phi\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of what the outputs should probably look like:\n",
    "\n",
    "Muon Kinematics\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"plots/processor_example_muonPt.png\" style=\"width: 300px;\"/> </td>    \n",
    "<td> <img src=\"plots/processor_example_muonEta.png\" style=\"width: 300px;\"/> </td>    \n",
    "<td> <img src=\"plots/processor_example_muonPhi.png\" style=\"width: 300px;\"/> </td>    \n",
    "</tr></table>\n",
    "\n",
    "Leading Jet Kinematics\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"plots/processor_example_leadingJetPt.png\" style=\"width: 300px;\"/> </td>    \n",
    "<td> <img src=\"plots/processor_example_leadingJetEta.png\" style=\"width: 300px;\"/> </td>    \n",
    "<td> <img src=\"plots/processor_example_leadingJetPhi.png\" style=\"width: 300px;\"/> </td>    \n",
    "</tr></table>\n",
    "\n",
    "Second Jet Kinematics\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"plots/processor_example_secondJetPt.png\" style=\"width: 300px;\"/> </td>    \n",
    "<td> <img src=\"plots/processor_example_secondJetEta.png\" style=\"width: 300px;\"/> </td>    \n",
    "<td> <img src=\"plots/processor_example_secondJetPhi.png\" style=\"width: 300px;\"/> </td>    \n",
    "</tr></table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### Z-boson selector\n",
    "\n",
    "In this step, we're going to try to find events from a Z boson.\n",
    "\n",
    "You are going to implement a selection looking for events with exactly two muons that have opposite charge.  These muons should pass the same 'tight' selections used in the previous notebook for selection.\n",
    "\n",
    "We are looking for events that have exactly two leptons of one flavor (either two electrons, or two muons) and where the two leptons have opposite charge (one electron and one positron, or one muon and one antimuon).\n",
    "\n",
    "Then, make the following plots:\n",
    " - $p_T$ of the leading muon in the event\n",
    " - Mass of the combination of the two leptons\n",
    " - Difference between the two leptons in eta\n",
    " - Difference between the two leptons in phi\n",
    " - Difference between the two leptons in R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Zselector(processor.ProcessorABC):\n",
    "    def __init__(self):\n",
    "        ### This function is where the histograms are defined and any other initialization happens\n",
    "        \n",
    "        ### Muon pt\n",
    "        #Declare an axis for the dataset\n",
    "        dataset_axis = hist.Cat(\"dataset\",\"Dataset\")\n",
    "        \n",
    "        #Declare an axis for the muon pt\n",
    "        pt_axis = hist.Bin(\"pt\",\"$p_{T}$ [GeV]\", 40, 0, 200)\n",
    "        eta_axis = hist.Bin(\"eta\",\"$\\eta$\", 40, -2.4, 2.4)\n",
    "        phi_axis = hist.Bin(\"phi\",\"$\\phi$\", 40, -3.142, 3.142)\n",
    "        mass_axis = hist.Bin(\"mass\",\"$m$ [GeV]\", 100, 0, 200)\n",
    "        dR_axis = hist.Bin(\"dR\",\"$\\Delta R$\", 100, 0, 5)\n",
    "        dEta_axis = hist.Bin(\"dEta\",\"$\\Delta \\eta$\", 100, 0, 5)\n",
    "        dPhi_axis = hist.Bin(\"dPhi\",\"$\\Delta \\phi$\", 100, 0, 3.14)\n",
    "\n",
    "        #Define the accumulator object, a dictionary storing all of the histograms and counters \n",
    "        #that we will fill later in the process function\n",
    "        self._accumulator = processor.dict_accumulator({\n",
    "            'muon_pt': hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            'muon_eta': hist.Hist(\"Counts\", dataset_axis,eta_axis),\n",
    "            'muon_phi': hist.Hist(\"Counts\", dataset_axis,phi_axis),\n",
    "            'dilep_mass': hist.Hist(\"Counts\", dataset_axis, mass_axis,),\n",
    "            'dilep_dR': hist.Hist(\"Counts\", dataset_axis, dR_axis),\n",
    "            'dilep_dEta': hist.Hist(\"Counts\", dataset_axis, dEta_axis),\n",
    "            'dilep_dPhi': hist.Hist(\"Counts\", dataset_axis, dPhi_axis),\n",
    "        }\n",
    "        )\n",
    "\n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "\n",
    "    def process(self, events):\n",
    "        ### The process function is where most of the work happens. As we'll see below, this is\n",
    "        ### where the main analysis work happens (object cuts, event selections, filling histograms). \n",
    "        \n",
    "        ## This gets us the accumulator dictionary we defined in init\n",
    "        output = self.accumulator.identity()\n",
    "\n",
    "        ## To access variables from the ntuples, use the \"events\" object\n",
    "        ## The dataset name is part of events.metadata\n",
    "        dataset = events.metadata['dataset']\n",
    "\n",
    "        trigger = events.HLT.IsoMu24 | events.HLT.IsoTkMu24\n",
    "        \n",
    "        muons = events.Muon\n",
    "        muonSelectTight = ((muons.pt>30) &\n",
    "                           (abs(muons.eta)<2.4) &\n",
    "                           (muons.tightId) &\n",
    "                           (muons.pfRelIso04_all < 0.15)\n",
    "                          )\n",
    "\n",
    "        tightMuons = muons[muonSelectTight]\n",
    "    \n",
    "        diMuons = tightMuons[(ak.num(tightMuons)==2)&trigger]\n",
    "        mu1 = diMuons[:,0]\n",
    "        mu2 = diMuons[:,1]\n",
    "\n",
    "        \n",
    "        \n",
    "        diMuons = ak.combinations(tightMuons,2)\n",
    "        mu1, mu2 = ak.unzip(diMuons)\n",
    "\n",
    "        eventSel = (((mu1+mu2).mass > 20) &\n",
    "                    ((mu1.charge + mu2.charge)==0))\n",
    "\n",
    "        \n",
    "        # Fill the muon_pt histogram using the tightMuons in events that pass our selection \n",
    "        # Note that ak.flatten() is required when filling a histogram to remove the jaggedness\n",
    "        output['muon_pt'].fill(dataset=dataset,\n",
    "                               pt=ak.flatten(mu1.pt[eventSel]),\n",
    "                               )\n",
    "                                       \n",
    "                \n",
    "        output['muon_eta'].fill(dataset=dataset,\n",
    "                               eta=ak.flatten(mu1.eta[eventSel]),\n",
    "                              )\n",
    "        output['muon_phi'].fill(dataset=dataset,\n",
    "                               phi=ak.flatten(mu1.phi[eventSel]),\n",
    "                              )\n",
    "\n",
    "        output['dilep_mass'].fill(dataset=dataset,\n",
    "                                  mass=ak.flatten((mu1+mu2).mass[eventSel]),\n",
    "                                 )\n",
    "        output['dilep_dR'].fill(dataset=dataset,\n",
    "                                  dR=ak.flatten(mu1.delta_r(mu2)[eventSel]),\n",
    "                                 )\n",
    "        output['dilep_dEta'].fill(dataset=dataset,\n",
    "                                  dEta=ak.flatten(abs(mu1.eta - mu2.eta)[eventSel]),\n",
    "                                 )\n",
    "        output['dilep_dPhi'].fill(dataset=dataset,\n",
    "                                  dPhi=ak.flatten(mu1.delta_phi(mu2)[eventSel]),\n",
    "                                 )\n",
    "        \n",
    "        \n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the NanoAODSchema needs to be adjusted, to remove cross references to FSRPhotons\n",
    "class SkimmedSchema(NanoAODSchema):\n",
    "    def __init__(self, base_form):\n",
    "        base_form[\"contents\"].pop(\"Muon_fsrPhotonIdx\", None)\n",
    "        super().__init__(base_form)\n",
    "\n",
    "#Run Coffea code using uproot\n",
    "outputMC_Z = processor.run_uproot_job(\n",
    "    fileset,  #dictionary of datasets to run on, defined earlier in this cell\n",
    "    \"Events\", #Name of the TTree you will be opening\n",
    "    Zselector(),  #Coffea processor you defined\n",
    "    processor.futures_executor,\n",
    "    executor_args={\"schema\": SkimmedSchema,'workers': 4},  ## workers = 2, parallelize jobs, running 2 at once\n",
    "    chunksize=1000000, #in each chunk, use 1 million events\n",
    "#     maxchunks=1, #limit to using only 3 chunks for each dataset (useful for testing purposes)\n",
    ")\n",
    "\n",
    "#Scale histograms\n",
    "for key, obj in outputMC_Z.items():\n",
    "    if isinstance(obj, hist.Hist):\n",
    "        obj.scale(lumi_weight, axis=\"dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputData_Z = processor.run_uproot_job(\n",
    "    filesetData, \n",
    "    \"Events\", \n",
    "    Zselector(), \n",
    "    processor.futures_executor,\n",
    "    executor_args={\"schema\": SkimmedSchema,'workers': 4},  \n",
    "    chunksize=1000000, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draw plots"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttgenv",
   "language": "python",
   "name": "ttgenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
